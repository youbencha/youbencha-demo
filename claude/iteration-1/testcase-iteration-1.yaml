# Example Test Case: Multiple Agentic Judge Evaluators
# 
# This demonstrates how to use multiple agentic-judge evaluators in a single test case.
# Each judge focuses on a specific aspect of code quality, keeping their context windows
# manageable and evaluations focused.
#
# Key benefits:
# - Each judge has 1-3 focused assertions instead of 10+ combined assertions
# - Cleaner separation of concerns (error handling, docs, tests, etc.)
# - Independent pass/fail status per evaluation area
# - Results clearly show which area passed/failed in reports

# Test case metadata
name: "Add error handling to API client"
description: "Tests the agent's ability to add comprehensive error handling to an API client with good practices"
workspace_name: claude-snake-iteration-1
# Repository configuration
repo: https://github.com/youbencha/hello-world.git
branch: demo

# Agent configuration
agent:
  type: claude-code
  agent_name: step-1-generic-agent
  config:
    prompt_file: ./iteration-1-prompt.md

# Evaluators - multiple agentic-judge instances with different focuses
evaluators:
  # Standard git-diff to track scope of changes
  - name: git-diff
    config:
      assertions:
        max_files_changed: 5
        max_total_changes: 200
  
  # Judge 1: Error Handling Quality
  # Focused evaluation of error handling implementation
  - name: agentic-judge-error-handling
    config:
      type: claude-code
      agent_name: agentic-judge
      timeout: 300000  # 5 minutes
      
      assertions:
        has_try_catch: "Code includes try-catch blocks for error handling. Score 1 if present, 0 if absent."
        error_messages_clear: "Error messages are descriptive and helpful. Score 1 if clear, 0.5 if vague, 0 if absent."
        errors_logged: "Errors are properly logged for debugging. Score 1 if logged, 0 if not."
  
  # Judge 2: Code Documentation
  # Separate evaluation of documentation quality
  - name: agentic-judge-documentation
    config:
      type: claude-code
      agent_name: agentic-judge
      timeout: 300000  # 5 minutes
      
      assertions:
        functions_documented: "Functions have JSDoc or similar documentation. Score 1 if all documented, 0.5 if partial, 0 if none."
        error_types_documented: "Error types and scenarios are documented. Score 1 if well documented, 0 if missing."
  
  # Judge 3: Best Practices
  # Evaluation of coding standards and practices
  - name: agentic-judge-best-practices
    config:
      type: claude-code
      agent_name: agentic-judge
      timeout: 300000  # 5 minutes
      
      assertions:
        no_console_log: "No console.log statements (use proper logging). Score 1 if clean, 0 if console.log found."
        proper_error_types: "Uses appropriate error types (not generic Error). Score 1 if specific types used, 0 if generic."
        follows_conventions: "Code follows project conventions and style. Score 1 if consistent, 0.5 if minor issues, 0 if inconsistent."

# Optional: Workspace configuration
workspace_dir: .youbencha-workspace
timeout: 300000  # 5 minutes

# Usage:
# yb run -c examples/testcase-multiple-judges.yaml
# yb report --from .youbencha-workspace/run-*/artifacts/results.json --format markdown
#
# The report will show:
# - agentic-judge-error-handling: 3 metrics (has_try_catch, error_messages_clear, errors_logged)
# - agentic-judge-documentation: 2 metrics (functions_documented, error_types_documented)
# - agentic-judge-best-practices: 3 metrics (no_console_log, proper_error_types, follows_conventions)
#
# Each evaluator has its own pass/fail status, making it easy to identify
# which specific area needs improvement.
